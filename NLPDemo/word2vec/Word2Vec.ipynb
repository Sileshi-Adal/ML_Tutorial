{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensim’s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Q01_Best part\\tQ02_Worst part\\tQ03_Time\\n'\n"
     ]
    }
   ],
   "source": [
    "data_file=\"Feedback1.csv\"\n",
    "\n",
    "with open (data_file, 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 15:08:19,529 : INFO : reading file Feedback1.csv...this may take a while\n",
      "2019-02-11 15:08:19,531 : INFO : read 0 reviews\n",
      "2019-02-11 15:08:19,630 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 15:08:19,642 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-02-11 15:08:19,650 : INFO : collecting all words and their counts\n",
      "2019-02-11 15:08:19,655 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-11 15:08:19,676 : INFO : collected 2755 word types from a corpus of 32794 raw words and 703 sentences\n",
      "2019-02-11 15:08:19,680 : INFO : Loading a fresh vocabulary\n",
      "2019-02-11 15:08:19,694 : INFO : effective_min_count=2 retains 1548 unique words (56% of original 2755, drops 1207)\n",
      "2019-02-11 15:08:19,695 : INFO : effective_min_count=2 leaves 31587 word corpus (96% of original 32794, drops 1207)\n",
      "2019-02-11 15:08:19,712 : INFO : deleting the raw counts dictionary of 2755 items\n",
      "2019-02-11 15:08:19,715 : INFO : sample=0.001 downsamples 77 most-common words\n",
      "2019-02-11 15:08:19,719 : INFO : downsampling leaves estimated 22207 word corpus (70.3% of prior 31587)\n",
      "2019-02-11 15:08:19,744 : INFO : estimated required memory for 1548 words and 150 dimensions: 2631600 bytes\n",
      "2019-02-11 15:08:19,747 : INFO : resetting layer weights\n",
      "2019-02-11 15:08:19,805 : INFO : training model with 10 workers on 1548 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-02-11 15:08:19,831 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:19,873 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:19,875 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:19,878 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:19,880 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:19,882 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:19,886 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:19,889 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:19,891 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:19,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:19,896 : INFO : EPOCH - 1 : training on 32794 raw words (22099 effective words) took 0.1s, 272142 effective words/s\n",
      "2019-02-11 15:08:19,912 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:19,946 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:19,948 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:19,951 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:19,953 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:19,955 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:19,958 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:19,960 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:19,962 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:19,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:19,966 : INFO : EPOCH - 2 : training on 32794 raw words (22164 effective words) took 0.1s, 412541 effective words/s\n",
      "2019-02-11 15:08:19,986 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,018 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,021 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,024 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,026 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,029 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,040 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,041 : INFO : EPOCH - 3 : training on 32794 raw words (22287 effective words) took 0.1s, 367695 effective words/s\n",
      "2019-02-11 15:08:20,064 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,085 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,092 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,093 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,096 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,099 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,100 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,106 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,108 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,110 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,110 : INFO : EPOCH - 4 : training on 32794 raw words (22257 effective words) took 0.1s, 382903 effective words/s\n",
      "2019-02-11 15:08:20,131 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,144 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,146 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,148 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,150 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,154 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,157 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,162 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,169 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,171 : INFO : EPOCH - 5 : training on 32794 raw words (22209 effective words) took 0.0s, 462939 effective words/s\n",
      "2019-02-11 15:08:20,175 : INFO : training on a 163970 raw words (111016 effective words) took 0.4s, 301423 effective words/s\n",
      "2019-02-11 15:08:20,178 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-02-11 15:08:20,182 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-02-11 15:08:20,186 : INFO : training model with 10 workers on 1548 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-02-11 15:08:20,212 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,225 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,227 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,228 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,229 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,231 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,238 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,249 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,252 : INFO : EPOCH - 1 : training on 32794 raw words (22138 effective words) took 0.1s, 391264 effective words/s\n",
      "2019-02-11 15:08:20,282 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,301 : INFO : worker thread finished; awaiting finish of 8 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 15:08:20,305 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,308 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,311 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,315 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,317 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,326 : INFO : EPOCH - 2 : training on 32794 raw words (22200 effective words) took 0.0s, 501095 effective words/s\n",
      "2019-02-11 15:08:20,341 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,373 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,375 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,377 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,380 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,381 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,383 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,385 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,392 : INFO : EPOCH - 3 : training on 32794 raw words (22197 effective words) took 0.1s, 437913 effective words/s\n",
      "2019-02-11 15:08:20,410 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,435 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,436 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,438 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,439 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,441 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,442 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,447 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,451 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,460 : INFO : EPOCH - 4 : training on 32794 raw words (22207 effective words) took 0.1s, 390149 effective words/s\n",
      "2019-02-11 15:08:20,477 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,488 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,508 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,510 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,512 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,514 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,516 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,522 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,524 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,525 : INFO : EPOCH - 5 : training on 32794 raw words (22092 effective words) took 0.0s, 458459 effective words/s\n",
      "2019-02-11 15:08:20,546 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,570 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,575 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,577 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,579 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,580 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,582 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,587 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,589 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,591 : INFO : EPOCH - 6 : training on 32794 raw words (22135 effective words) took 0.1s, 433474 effective words/s\n",
      "2019-02-11 15:08:20,612 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,632 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,641 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,643 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,644 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,649 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,652 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,657 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,659 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,661 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,662 : INFO : EPOCH - 7 : training on 32794 raw words (22185 effective words) took 0.1s, 391247 effective words/s\n",
      "2019-02-11 15:08:20,683 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,711 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,714 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,715 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,721 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,724 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,726 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,730 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,733 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,735 : INFO : EPOCH - 8 : training on 32794 raw words (22246 effective words) took 0.1s, 388918 effective words/s\n",
      "2019-02-11 15:08:20,751 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-02-11 15:08:20,778 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,780 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,781 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,785 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,789 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,792 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,793 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,800 : INFO : EPOCH - 9 : training on 32794 raw words (22158 effective words) took 0.1s, 401705 effective words/s\n",
      "2019-02-11 15:08:20,826 : INFO : worker thread finished; awaiting finish of 9 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-11 15:08:20,845 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-02-11 15:08:20,847 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-02-11 15:08:20,848 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-02-11 15:08:20,850 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-02-11 15:08:20,853 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-02-11 15:08:20,855 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-11 15:08:20,857 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-11 15:08:20,859 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-11 15:08:20,862 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-11 15:08:20,865 : INFO : EPOCH - 10 : training on 32794 raw words (22309 effective words) took 0.1s, 419251 effective words/s\n",
      "2019-02-11 15:08:20,868 : INFO : training on a 327940 raw words (221867 effective words) took 0.7s, 325710 effective words/s\n",
      "2019-02-11 15:08:20,870 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(221867, 327940)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('features', 0.9895908236503601),\n",
       " ('function', 0.9875247478485107),\n",
       " ('labels', 0.9863935112953186),\n",
       " ('hypothesis', 0.9808359146118164),\n",
       " ('theoretically', 0.9803018569946289),\n",
       " ('dealing', 0.9787272810935974),\n",
       " ('suitable', 0.9762242436408997),\n",
       " ('explained', 0.973731517791748),\n",
       " ('hardest', 0.9730471968650818),\n",
       " ('choosing', 0.9728363156318665)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"loss\"\n",
    "model.wv.most_similar (positive=w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('important', 0.9984394311904907),\n",
       " ('basics', 0.9983464479446411),\n",
       " ('practical', 0.9983209371566772),\n",
       " ('gave', 0.9982707500457764),\n",
       " ('especially', 0.9979274272918701),\n",
       " ('enjoyed', 0.997899055480957)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"learn\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'france' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-787e423bbcaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# look up top 6 words similar to 'france'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"france\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'france' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"help\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"matrix\",w2=\"math\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"matrix\",w2=\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec=model.wv['lecture']\n",
    "print(vec[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"matrix\",\"linear\",\"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
